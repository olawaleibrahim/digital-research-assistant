{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure the search\n",
    "AUTHOR_NAME = \"Lei Liang\"  # Replace with the author's name\n",
    "SAVE_DIR = \"papers\"  # Directory to save the downloaded PDFs\n",
    "\n",
    "# Google Scholar search URL (or use other academic search engines)\n",
    "BASE_URL = \"https://paperswithcode.com/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def search_papers(author_name):\n",
    "    \"\"\"Search for papers by author name.\"\"\"\n",
    "    params = {\"q\": f'author:\"{author_name}\"', \"hl\": \"en\"}\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def extract_paper_links(html):\n",
    "    \"\"\"Extract links to PDFs or paper details.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for result in soup.select(\".gs_r.gs_or\"):\n",
    "        pdf_link = result.find(\"a\", href=True)\n",
    "        if pdf_link and pdf_link[\"href\"].endswith(\".pdf\"):\n",
    "            links.append(pdf_link[\"href\"])\n",
    "    return links\n",
    "\n",
    "def download_paper(url, save_dir):\n",
    "    \"\"\"Download a PDF given its URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, stream=True)\n",
    "        response.raise_for_status()\n",
    "        filename = re.sub(r'[^\\w\\-_\\. ]', '_', url.split(\"/\")[-1])  # Clean the filename\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"Searching for papers by {AUTHOR_NAME}...\")\n",
    "    html = search_papers(AUTHOR_NAME)\n",
    "    paper_links = extract_paper_links(html)\n",
    "    \n",
    "    if not paper_links:\n",
    "        print(\"No PDF links found. Consider using a different source or API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(paper_links)} papers. Downloading...\")\n",
    "    for link in paper_links:\n",
    "        download_paper(link, SAVE_DIR)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers by Lei Liang...\n",
      "No PDF links found. Consider using a different source or API.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from cleantext import clean\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text by removing extra spaces, newlines, and special characters.\"\"\"\n",
    "    # Remove newlines, tabs, and multiple spaces\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s.,!?]\", \" \", text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    \n",
    "    # Use cleantext to further process the text\n",
    "    # cleaned_text = clean(\n",
    "    #     cleaned_text,\n",
    "    #     lowercase=True,           # Convert to lowercase\n",
    "    #     extra_spaces=True,         # Remove URLs\n",
    "    #     stemming=True,             # Remove emojis\n",
    "    # )\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/notebooks/papers/s11248-015-9867-7.pdf\"  # Replace with your PDF file path\n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"\\nRaw Extracted Text:\")\n",
    "    print(raw_text[:500])  # Print first 500 characters for preview\n",
    "    \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    print(\"\\nCleaned Text:\")\n",
    "    print(cleaned_text[:500])  # Print first 500 characters for preview\n",
    "    \n",
    "    # Optionally, save cleaned text to a file\n",
    "    with open(\"cleaned_text.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(cleaned_text)\n",
    "        print(\"\\nCleaned text saved to 'cleaned_text.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/notebooks/papers/s11248-015-9867-7.pdf...\n",
      "\n",
      "Raw Extracted Text:\n",
      "REVIEW\n",
      "Genetic basis and detection of unintended effects\n",
      "in genetically modiﬁed crop plants\n",
      "Gregory S. Ladics •Andrew Bartholomaeus •Phil Bregitzer •Nancy G. Doerrer •\n",
      "Alan Gray •Thomas Holzhauser •Mark Jordan •Paul Keese •Esther Kok •Phil Macdonald •\n",
      "Wayne Parrott •Laura Privalle •Alan Raybould •Seung Yon Rhee •Elena Rice •\n",
      "Jo¨rg Romeis •Justin Vaughn •Jean-Michel Wal •Kevin Glenn\n",
      "Received: 18 January 2015 / Accepted: 14 February 2015 / Published online: 26 February 2015\n",
      "/C211The Author(s) 2015\n",
      "\n",
      "Cleaned Text:\n",
      "REVIEW Genetic basis and detection of unintended effects in genetically modiﬁed crop plants Gregory S. Ladics Andrew Bartholomaeus Phil Bregitzer Nancy G. Doerrer Alan Gray Thomas Holzhauser Mark Jordan Paul Keese Esther Kok Phil Macdonald Wayne Parrott Laura Privalle Alan Raybould Seung Yon Rhee Elena Rice Jo rg Romeis Justin Vaughn Jean Michel Wal Kevin Glenn Received 18 January 2015 Accepted 14 February 2015 Published online 26 February 2015 C211The Author s 2015. This article is published wi\n",
      "\n",
      "Cleaned text saved to 'cleaned_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2310.11511v1.pdf', '2312.10997v5.pdf', '2005.11401v4.pdf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/\"\n",
    "username = \"olawale_ibrahim\"\n",
    "filepaths = ROOT_DIR + username + \"/\"\n",
    "os.listdir(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/digital_research_assistant/data/input/olawale_ibrahim/',\n",
       " '/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/olawale_ibrahim')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths, \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/olawale_ibrahim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
