{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure the search\n",
    "AUTHOR_NAME = \"Lei Liang\"  # Replace with the author's name\n",
    "SAVE_DIR = \"papers\"  # Directory to save the downloaded PDFs\n",
    "\n",
    "# Google Scholar search URL (or use other academic search engines)\n",
    "BASE_URL = \"https://paperswithcode.com/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def search_papers(author_name):\n",
    "    \"\"\"Search for papers by author name.\"\"\"\n",
    "    params = {\"q\": f'author:\"{author_name}\"', \"hl\": \"en\"}\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def extract_paper_links(html):\n",
    "    \"\"\"Extract links to PDFs or paper details.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for result in soup.select(\".gs_r.gs_or\"):\n",
    "        pdf_link = result.find(\"a\", href=True)\n",
    "        if pdf_link and pdf_link[\"href\"].endswith(\".pdf\"):\n",
    "            links.append(pdf_link[\"href\"])\n",
    "    return links\n",
    "\n",
    "def download_paper(url, save_dir):\n",
    "    \"\"\"Download a PDF given its URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, stream=True)\n",
    "        response.raise_for_status()\n",
    "        filename = re.sub(r'[^\\w\\-_\\. ]', '_', url.split(\"/\")[-1])  # Clean the filename\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"Searching for papers by {AUTHOR_NAME}...\")\n",
    "    html = search_papers(AUTHOR_NAME)\n",
    "    paper_links = extract_paper_links(html)\n",
    "    \n",
    "    if not paper_links:\n",
    "        print(\"No PDF links found. Consider using a different source or API.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(paper_links)} papers. Downloading...\")\n",
    "    for link in paper_links:\n",
    "        download_paper(link, SAVE_DIR)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers by Lei Liang...\n",
      "No PDF links found. Consider using a different source or API.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "from cleantext import clean\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text by removing extra spaces, newlines, and special characters.\"\"\"\n",
    "    # Remove newlines, tabs, and multiple spaces\n",
    "    cleaned_text = re.sub(r\"[^\\w\\s.,!?]\", \" \", text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    \n",
    "    # Use cleantext to further process the text\n",
    "    # cleaned_text = clean(\n",
    "    #     cleaned_text,\n",
    "    #     lowercase=True,           # Convert to lowercase\n",
    "    #     extra_spaces=True,         # Remove URLs\n",
    "    #     stemming=True,             # Remove emojis\n",
    "    # )\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/notebooks/papers/s11248-015-9867-7.pdf\"  # Replace with your PDF file path\n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"\\nRaw Extracted Text:\")\n",
    "    print(raw_text[:500])  # Print first 500 characters for preview\n",
    "    \n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    print(\"\\nCleaned Text:\")\n",
    "    print(cleaned_text[:500])  # Print first 500 characters for preview\n",
    "    \n",
    "    # Optionally, save cleaned text to a file\n",
    "    with open(\"cleaned_text.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(cleaned_text)\n",
    "        print(\"\\nCleaned text saved to 'cleaned_text.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/notebooks/papers/s11248-015-9867-7.pdf...\n",
      "\n",
      "Raw Extracted Text:\n",
      "REVIEW\n",
      "Genetic basis and detection of unintended effects\n",
      "in genetically modiﬁed crop plants\n",
      "Gregory S. Ladics •Andrew Bartholomaeus •Phil Bregitzer •Nancy G. Doerrer •\n",
      "Alan Gray •Thomas Holzhauser •Mark Jordan •Paul Keese •Esther Kok •Phil Macdonald •\n",
      "Wayne Parrott •Laura Privalle •Alan Raybould •Seung Yon Rhee •Elena Rice •\n",
      "Jo¨rg Romeis •Justin Vaughn •Jean-Michel Wal •Kevin Glenn\n",
      "Received: 18 January 2015 / Accepted: 14 February 2015 / Published online: 26 February 2015\n",
      "/C211The Author(s) 2015\n",
      "\n",
      "Cleaned Text:\n",
      "REVIEW Genetic basis and detection of unintended effects in genetically modiﬁed crop plants Gregory S. Ladics Andrew Bartholomaeus Phil Bregitzer Nancy G. Doerrer Alan Gray Thomas Holzhauser Mark Jordan Paul Keese Esther Kok Phil Macdonald Wayne Parrott Laura Privalle Alan Raybould Seung Yon Rhee Elena Rice Jo rg Romeis Justin Vaughn Jean Michel Wal Kevin Glenn Received 18 January 2015 Accepted 14 February 2015 Published online 26 February 2015 C211The Author s 2015. This article is published wi\n",
      "\n",
      "Cleaned text saved to 'cleaned_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2310.11511v1.pdf', '2312.10997v5.pdf', '2005.11401v4.pdf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/\"\n",
    "username = \"olawale_ibrahim\"\n",
    "filepaths = ROOT_DIR + username + \"/\"\n",
    "os.listdir(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/digital_research_assistant/data/input/olawale_ibrahim/',\n",
       " '/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/olawale_ibrahim')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths, \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/olawale_ibrahim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/olawale/Desktop/PROJECTS/llms/self-rag/llm-venv/lib/python3.11/site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/olawale/Desktop/PROJECTS/llms/self-rag/llm-venv/lib/python3.11/site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olawale holds a B.Tech. in Applied Geophysics (first class honors) from the Federal University of Technology, Nigeria. He is a seasoned geoscience software developer with over four years of experience working with oil and gas companies across the world. He specializes in helping geoscience teams adopt latest digital tools like AI and machine learning in improving and automating traditional workflows to better save time, cost and manpower labor, which consequently improves efficiency and generates better results. \n",
      "In his latest role as a machine learning engineer at CGG (UK), He leads current R&D efforts and advancements by spearheading research, developing and putting analytical and digital tools in the hands of experienced geoscientists to make quicker and insightful interpretations. At Earth Science Analytics (Norway), he developed machine learning models for both seismic processing, interpretation and petrophysical applications. As a machine learning consultant with dGB Earth Sciences (the Netherlands), he worked on OpendTect’s machine learning plugin by improving existing seismic and well log data interpretation by working with inhouse software experts. He has also won two best paper awards at NAPE AICE for his work on the application of machine learning models for well logs prediction and seismic salt bodies identification. His core interests lie in the intersection of geoscience, artificial intelligence and software development.\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_word(doc_path):\n",
    "    \"\"\"Extract text from a Word document.\"\"\"\n",
    "    document = Document(doc_path)\n",
    "    text = \"\"\n",
    "    for paragraph in document.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"  # Add newlines to separate paragraphs\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage\n",
    "doc_path = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/data/input/olawale_ibrahim/Olawale Ibrahim Bio.docx\"  # Replace with your Word document's path\n",
    "extracted_text = extract_text_from_word(doc_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
