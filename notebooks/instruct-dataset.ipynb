{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "from datasets import Dataset\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc_path = \"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/output/cleaned_documents.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_from_json(file_path: str) -> Dataset:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "        \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n",
    "        \"content\": [item[\"content\"] for item in data[\"artifact_data\"]],\n",
    "        \"filetype\": [item[\"filetype\"] for item in data[\"artifact_data\"]],\n",
    "        \"author_id\": [item[\"author_id\"] for item in data[\"artifact_data\"]],\n",
    "        \"author_full_name\": [item[\"author_full_name\"] for item in data[\"artifact_data\"]],\n",
    "        \"filepath\": [item[\"filepath\"] for item in data[\"artifact_data\"]],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_substrings(dataset: Dataset, min_length: int = 1000,\n",
    "    max_length: int = 2000) -> List[str]:\n",
    "    extracts = []\n",
    "    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\"\n",
    "    for article in dataset[\"content\"]:\n",
    "        cleaned_article = clean_text(article)\n",
    "        sentences = re.split(sentence_pattern, cleaned_article)\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            if len(current_chunk) + len(sentence) <= max_length:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                if len(current_chunk) >= min_length:\n",
    "                    extracts.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        if len(current_chunk) >= min_length:\n",
    "            extracts.append(current_chunk.strip())\n",
    "    return extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionAnswerSet:\n",
    "    def __init__(self, pairs: List[Tuple[str, str]]):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'InstructionAnswerSet':\n",
    "        data = json.loads(json_str)\n",
    "        pairs = [(pair['instruction'], pair['answer']) for pair in data['instruction_answer_pairs']]\n",
    "        return cls(pairs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_answer_pairs(\n",
    "    extract: str, client: OpenAI\n",
    "    ) -> List[Tuple[str, str]]:\n",
    "    prompt = f\"\"\"Based on the following extract, generate five\n",
    "    instruction-answer pairs. Each instruction \\\n",
    "    must ask to write about a specific topic contained in the context.\n",
    "    each answer \\\n",
    "    must provide a relevant paragraph based on the information found in\n",
    "    the \\\n",
    "    context. Only use concepts from the context to generate the\n",
    "    instructions. \\\n",
    "    Instructions must never explicitly mention a context, a system, a\n",
    "    course, or an extract. \\\n",
    "    Instructions must be self-contained and general. \\\n",
    "    Answers must imitate the writing style of the context. \\\n",
    "    Example instruction: Explain the concept of an LLM Twin. \\\n",
    "    Example answer: An LLM Twin is essentially an AI character that\n",
    "    mimics your writing style, personality, and voice. \\\n",
    "    It's designed to write just like you by incorporating these elements\n",
    "    into a language model. \\\n",
    "    The idea is to create a digital replica of your writing habits using\n",
    "    advanced AI techniques. \\\n",
    "    Provide your response in JSON format with the following structure:\n",
    "    {{\n",
    "    \"instruction_answer_pairs\": [\n",
    "    {{\"instruction\": \"...\", \"answer\": \"...\"}},\n",
    "    ...\n",
    "    ]\n",
    "    }}\n",
    "    Extract:\n",
    "    {extract}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \"content\": \"You are a helpfulassistant who \\\n",
    "                        generates instruction-answer pairs based on the given context. \\\n",
    "                            Provide your response in JSON format.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            max_tokens=1200,\n",
    "            temperature=0.7,\n",
    "            )\n",
    "    \n",
    "    result = InstructionAnswerSet.from_json(completion.choices[0].\n",
    "    message.content)\n",
    "    \n",
    "    return result.pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instruction_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -> Dataset:\n",
    "    extracts = extract_substrings(dataset)\n",
    "    instruction_answer_pairs = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(generate_instruction_answer_pairs, extract, client) for extract in extracts]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures),total=len(futures)):\n",
    "            instruction_answer_pairs.extend(future.result())\n",
    "            instructions, answers = zip(*instruction_answer_pairs)\n",
    "    return Dataset.from_dict(\n",
    "    {\"instruction\": list(instructions), \"output\": list(answers)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_id: str) -> Dataset:\n",
    "    client = OpenAI()\n",
    "    raw_dataset = load_articles_from_json(clean_doc_path)\n",
    "    print(\"Raw dataset:\")\n",
    "\n",
    "    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n",
    "    print(\"Instruction dataset:\")\n",
    "    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n",
    "    filtered_dataset.push_to_hub(f\"olawaleibrahim/{dataset_id}\", private=True)\n",
    "    return filtered_dataset, instruction_dataset.to_pandas(), raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"professionaldocuments\"\n",
    "# dataset_filtered.push_to_hub(f\"olawaleibrahim/{dataset_id}\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5c6c7b501540d88facb8ffb8f59ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcde99fd178d4a9f9ad1229782fa6d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed39bbfa1bac4ef39e75d332ce9a14c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6992e682e7ad446f8e9236e9f48c3f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c268d2a6ce74bb68d175328b0193296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/407 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/olawaleibrahim/1professionaldocuments/commit/e94194ec147e9eb868fdc4ecc44355f95218f751', commit_message='Upload dataset', commit_description='', oid='e94194ec147e9eb868fdc4ecc44355f95218f751', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/olawaleibrahim/1professionaldocuments', endpoint='https://huggingface.co', repo_type='dataset', repo_id='olawaleibrahim/1professionaldocuments'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered.push_to_hub(f\"olawaleibrahim/1{dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e3f42a2da24283a70418a8a341c683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cf2be33b7049928306248d91204349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edfa6bbc24a4de3b897cee076bda0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947d49ab56ea47a18e10f6724445b946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c5c176daf4df68d5786266e1927c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_filtered, df, raw_dataset = main(\"professionaldocuments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 1386\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/olawale/Desktop/PROJECTS/llms/digital-research-assistant/output/clean_documents_hf.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
